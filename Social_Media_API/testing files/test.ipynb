{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install facebook-scraper\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<' not supported between instances of 'NoneType' and 'datetime.datetime'\n",
      "HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Opus\\AppData\\Local\\Temp\\ipykernel_12152\\2575265351.py:46: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('C:/Users/Opus/chromedriver.exe', chrome_options=chrome_options)\n",
      "C:\\Users\\Opus\\AppData\\Local\\Temp\\ipykernel_12152\\2575265351.py:46: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome('C:/Users/Opus/chromedriver.exe', chrome_options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0                                         4.9K views   \n",
      "1  Doctor explains why the FDA delayed emergency ...   \n",
      "2  Who Got the Covid Relief Money? | Real Time wi...   \n",
      "\n",
      "                                         Description  \\\n",
      "0                                                New   \n",
      "1  Dr. Peter Hotez, co-director of the Texas Chil...   \n",
      "2  Subscribe to the Real Time YouTube: http://its...   \n",
      "\n",
      "                                            Duration        Views  \\\n",
      "0  COVID-19 exposure concerns for Queen Elizabeth...  4 hours ago   \n",
      "1  Doctor explains why the FDA delayed emergency ...    22K views   \n",
      "2                                               3:52    22K views   \n",
      "\n",
      "                                             Channel     Uploaded  \n",
      "0  Prince Charles tested positive for COVID-19 fo...     ABC News  \n",
      "1                                                CNN  4 hours ago  \n",
      "2                          Real Time with Bill Maher  2 hours ago  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Opus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\facebook_scraper\\facebook_scraper.py:791: UserWarning: Facebook served mbasic/noscript content unexpectedly on https://m.facebook.com/timeline/app_collection/?collection_token=100006908916880%3A103382489711013%3A33&_rdr\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv \n",
    "from getpass import getpass \n",
    "from time import sleep \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "import os\n",
    "import selenium\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import time\n",
    "import pandas as pd\n",
    "from GoogleNews import GoogleNews\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from facebook_scraper import get_profile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def YouTube(name):\n",
    "    \n",
    "\n",
    "\n",
    "    # create instance of Chrome webdriver\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome('C:/Users/Opus/chromedriver.exe', chrome_options=chrome_options)\n",
    "    text= name\n",
    "    driver.get('https://www.youtube.com/results?search_query='+ text)\n",
    "    contents = driver.find_elements(By.XPATH, '//*[@id=\"dismissible\"]')\n",
    "\n",
    "    duration_lst=[]\n",
    "    title_lst=[]\n",
    "    views_lst=[]\n",
    "    uploaded_lst=[]\n",
    "    channel_lst=[]\n",
    "    description_lst=[]\n",
    "    for content in contents:\n",
    "        try:\n",
    "            duration=content.text.split('\\n')[0]\n",
    "            title=content.text.split('\\n')[1]\n",
    "            views=content.text.split('\\n')[2]\n",
    "            uploaded=content.text.split('\\n')[3]\n",
    "            channel=content.text.split('\\n')[4]\n",
    "            description=content.text.split('\\n')[5]\n",
    "        except:\n",
    "            continue\n",
    "        duration_lst.append(duration)\n",
    "        title_lst.append(title)\n",
    "        views_lst.append(views)\n",
    "        uploaded_lst.append(uploaded)\n",
    "        channel_lst.append(channel)\n",
    "        description_lst.append(description)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'Title':title_lst, \n",
    "        'Description':description_lst,\n",
    "        'Duration':duration_lst,\n",
    "        'Views':views_lst,\n",
    "        'Channel':channel_lst,\n",
    "        'Uploaded':uploaded_lst\n",
    "        })\n",
    "    print(df)\n",
    "    df.to_csv(\"YouTube_1.csv\", index=False)\n",
    "    \n",
    "\n",
    "def google(Name):\n",
    "    date_lst =[]\n",
    "    title_lst =[]\n",
    "    media_lst =[]\n",
    "    datetime_lst =[]\n",
    "    desc_lst =[]\n",
    "    link_lst =[]\n",
    "    img_lst =[]\n",
    "\n",
    "\n",
    "    def show_routine(results):\n",
    "        for num,page in enumerate(results):\n",
    "            #print(f\"{num}. {page['date']} - {page['title']} - {page['media']} - {page['datetime']} - {page['desc']} - {page['link']} - {page['img']}  \")\n",
    "            date_lst.append(page['date'])\n",
    "            title_lst.append(page['title'])\n",
    "            media_lst.append(page['media'])\n",
    "            datetime_lst.append(page['datetime'])\n",
    "            desc_lst.append(page['desc'])\n",
    "            link_lst.append(page['link'])\n",
    "            img_lst.append(page['img'])\n",
    "\n",
    "    ### MAIN\n",
    "\n",
    "    # Setup the research\n",
    "    keywords=Name\n",
    "    period='10d'\n",
    "    google_news = GoogleNews(lang='en',period=period)\n",
    "    google=GoogleNews(lang='en',period=period)\n",
    "\n",
    "    # Results from news.google.com\n",
    "    google_news.get_news(keywords)\n",
    "    results_gnews=google_news.results(sort=True)\n",
    "    show_routine(results_gnews)\n",
    "\n",
    "    # Results from google.com\n",
    "    google.search(keywords)\n",
    "    results_google=google.results(sort=True)\n",
    "    show_routine(results_google)\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'date':date_lst, \n",
    "        'title':title_lst,\n",
    "        'media':media_lst,\n",
    "        'datetime':datetime_lst,\n",
    "        'desc':desc_lst,\n",
    "        'link':link_lst,\n",
    "        'img':img_lst\n",
    "        })\n",
    "    #print(df)\n",
    "    df.to_csv(\"google.csv\", index = False, header = True)\n",
    "    return df\n",
    "\n",
    "def twitter(name):\n",
    "    consumer_key        = 'sx6llfl3gRsOBNw3y7VVKSQT7'\n",
    "    consumer_secret     = 'DSZiX1mHEsKTcT7JMnahQY2dPC4lQNV21hkorPszWkz8D2Q5oH'\n",
    "    access_token        = '998804133288144901-pPeZ4V3B9jmEDhhVbvML9tYAMgXy55A'\n",
    "    access_token_secret = 'AdqyikZTdUaStqYAF4hKzCpRwSiLHWveOjlygarnrFIBY'\n",
    "\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tw.API(auth,wait_on_rate_limit = True)\n",
    "\n",
    "    search_words = name\n",
    "    new_search   = search_words + \" -filter:retweets\"\n",
    "\n",
    "    tweets = tw.Cursor(api.search_tweets, \n",
    "                    q                = new_search,\n",
    "                    result_type      = \"mixed\",\n",
    "                    lang             = \"id\",\n",
    "                    count            = 100,\n",
    "                    include_entities = True,\n",
    "                    since_id         = \"2020-01-01\").items(500)\n",
    "\n",
    "    users_locs = [[tweet.created_at,\n",
    "                tweet.author.screen_name,\n",
    "                tweet.author.name,\n",
    "                tweet.text,\n",
    "                tweet.retweet_count,\n",
    "                tweet.favorite_count,\n",
    "                tweet.user.location] for tweet in tweets]\n",
    "\n",
    "    tweet_text = pd.DataFrame(data = users_locs, \n",
    "                            columns = [\"date\",\n",
    "                                        \"user\",\n",
    "                                        \"name\",\n",
    "                                        \"text\",\n",
    "                                        \"retweet\",\n",
    "                                        \"favorite\",\n",
    "                                        \"location\"])\n",
    "\n",
    "    tweet_text.to_csv(\"twitter.csv\", index = False, header = True)\n",
    "    #tweet_text.to_excel(\"{}.xlsx\".format(search_words), index = False)\n",
    "    #print(tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "def Facebook(name):\n",
    "    # Initialize dataframe to scrape Facebook post\n",
    "    pro_df_full = pd.DataFrame(columns = [])\n",
    "    text = name\n",
    "    profile = get_profile(text, cookies=\"cookie.txt\")\n",
    "\n",
    "    fb_pro_df = pd.DataFrame.from_dict(profile, orient='index')\n",
    "    fb_pro_df = fb_pro_df.transpose()\n",
    "    pro_df_full = pro_df_full.append(fb_pro_df)\n",
    "    pro_df_full.head(50)\n",
    "    pro_df_full.to_csv(\"facebook.csv\", index= False)\n",
    "\n",
    "\n",
    "google(\"covid\")\n",
    "twitter(\"covid\")\n",
    "YouTube(\"covid\")\n",
    "Facebook(\"Shaon.ComputerGeek\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Opus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\facebook_scraper\\facebook_scraper.py:791: UserWarning: Facebook served mbasic/noscript content unexpectedly on https://m.facebook.com/timeline/app_collection/?collection_token=100006908916880%3A103382489711013%3A33&_rdr\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from facebook_scraper import get_profile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def Facebook(name):\n",
    "    # Initialize dataframe to scrape Facebook post\n",
    "    pro_df_full = pd.DataFrame(columns = [])\n",
    "    text = name\n",
    "    profile = get_profile(text, cookies=\"cookie.txt\")\n",
    "\n",
    "    fb_pro_df = pd.DataFrame.from_dict(profile, orient='index')\n",
    "    fb_pro_df = fb_pro_df.transpose()\n",
    "    pro_df_full = pro_df_full.append(fb_pro_df)\n",
    "    pro_df_full.head(50)\n",
    "    pro_df_full.to_csv(\"facebook.csv\", index= False)\n",
    "\n",
    "Facebook(\"Shaon.ComputerGeek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium-related\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# other necessary ones\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options as you wish\n",
    "option = Options()\n",
    "option.add_argument(\"--disable-infobars\")\n",
    "option.add_argument(\"start-maximized\")\n",
    "option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "with open('facebook_credentials.txt') as file:\n",
    "    EMAIL = file.readline().split('\"')[1]\n",
    "    PASSWORD = file.readline().split('\"')[1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Opus\\AppData\\Local\\Temp\\ipykernel_14480\\4180316274.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(executable_path=\"chromedriver.exe\", options=option)\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Chrome(executable_path=\"chromedriver.exe\", options=option)\n",
    "browser.get(\"http://facebook.com\")\n",
    "browser.maximize_window()\n",
    "wait = WebDriverWait(browser, 30)\n",
    "email_field = wait.until(EC.visibility_of_element_located((By.NAME, 'email')))\n",
    "email_field.send_keys(EMAIL)\n",
    "pass_field = wait.until(EC.visibility_of_element_located((By.NAME, 'pass')))\n",
    "pass_field.send_keys(PASSWORD)\n",
    "pass_field.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "text=\"opus%20technology%20limited\"\n",
    "\n",
    "browser.get('https://www.facebook.com/search/top?q='+ text) # once logged in, free to open up any target page\n",
    "\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "switch = True\n",
    "old_numReviews = 0\n",
    "specifiedNumber = 9 # number of reviews to get\n",
    "\n",
    "\n",
    "def openComment(browser):    \n",
    "    moreComment = browser.find_elements(By.XPATH, \"//span[contains(@class,'d2edcug0 hpfvmrgz qv66sw1b c1et5uql oi732d6d ik7dh3pa ht8s03o8 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d9wwppkn fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v lrazzd5p m9osqain') and starts-with(text(), 'View') and contains(text(), 'more comment')]\")\n",
    "    if len(moreComment) > 0:\n",
    "        count = 0\n",
    "        for i in moreComment:\n",
    "            action=ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element(i).click().perform()\n",
    "                count += 1\n",
    "            except:\n",
    "                try:\n",
    "                    browser.execute_script(\"arguments[0].click();\", i)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        if len(moreComment) - count > 0:\n",
    "            print('moreComment issue:', len(moreComment) - count)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "      \n",
    "def openReply(browser):\n",
    "    replies = browser.find_elements(By.XPATH, \"//div[@class='rq0escxv l9j0dhe7 du4w35lb j83agx80 cbu4d94t pfnyh3mw d2edcug0 hpfvmrgz n8tt0mok hyh9befq r8blr3vg jwdofwj8 g0qnabr5']\")\n",
    "    if len(replies) > 0:\n",
    "        count = 0\n",
    "        for i in replies:\n",
    "            action=ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element(i).click().perform()\n",
    "                count += 1\n",
    "            except:\n",
    "                try:\n",
    "                    browser.execute_script(\"arguments[0].click();\", i)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        if len(replies) - count > 0:\n",
    "            print('replies issue:', len(replies) - count)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def openReply2(browser):\n",
    "    replies = browser.find_elements(By.XPATH, \"//span[@class='d2edcug0 hpfvmrgz qv66sw1b c1et5uql oi732d6d ik7dh3pa ht8s03o8 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d9wwppkn fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v lrazzd5p m9osqain' and contains(text(),'more repl')]\")\n",
    "    if len(replies) > 0: \n",
    "        count = 0\n",
    "        for i in replies:\n",
    "            action=ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element(i).click().perform()\n",
    "                count += 1\n",
    "            except:\n",
    "                try:\n",
    "                    browser.execute_script(\"arguments[0].click();\", i)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        if len(replies) - count > 0:\n",
    "            print('replies2 issue:', len(replies) - count)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "      \n",
    "def openSeeMore(browser):\n",
    "    readMore = browser.find_elements(By.XPATH, \"//div[contains(@class,'oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl oo9gr5id gpro0wi8 lrazzd5p') and contains(text(), 'See More')]\")\n",
    "    if len(readMore) > 0:    \n",
    "        count = 0\n",
    "        for i in readMore:\n",
    "            action=ActionChains(browser)\n",
    "            try:\n",
    "                action.move_to_element(i).click().perform()\n",
    "                count += 1\n",
    "            except:\n",
    "                try:\n",
    "                    browser.execute_script(\"arguments[0].click();\", i)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        if len(readMore) - count > 0:\n",
    "            print('readMore issue:', len(readMore) - count)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def getBack(browser):\n",
    "    if not browser.current_url.endswith('reviews'):\n",
    "        print('redirected!!!')\n",
    "        browser.back()\n",
    "        print('got back!!!')\n",
    "\n",
    "def archiveAtEnd(browser, reviewList):\n",
    "    browser.execute_script(\"window.scrollTo(0, -document.body.scrollHeight);\") # scroll back to the top\n",
    "    time.sleep(10)\n",
    "        \n",
    "    for idx, l in enumerate(reviewList):\n",
    "        if idx % 10 == 0:\n",
    "            if idx < 15:\n",
    "                browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[0])\n",
    "            else:\n",
    "                browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[idx-15])\n",
    "            \n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[idx+15])\n",
    "            except:\n",
    "                browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[-1])\n",
    "\n",
    "            time.sleep(1)\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[idx])\n",
    "            \n",
    "            for r in range(2):\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[idx+5])\n",
    "                    time.sleep(3)\n",
    "                except:\n",
    "                    browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[-1])\n",
    "                browser.execute_script(\"arguments[0].scrollIntoView();\", reviewList[idx+r*3])\n",
    "                time.sleep(3)\n",
    "                with open(f'./PATH/{str(idx)}_{r}.html',\"w\", encoding=\"utf-8\") as file:\n",
    "                    source_data = browser.page_source\n",
    "                    bs_data = bs(source_data, 'html.parser')\n",
    "                    file.write(str(bs_data.prettify()))\n",
    "                    print(f'written: {idx}_{r}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Opus\\AppData\\Local\\Temp\\ipykernel_14480\\2527647080.py:23: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  reviewList = browser.find_elements_by_xpath(\"//div[@class='du4w35lb k4urcfbm l9j0dhe7 sjgh65i0']\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n",
      "redirected!!!\n",
      "got back!!!\n"
     ]
    }
   ],
   "source": [
    "while switch:\n",
    "    count += 1\n",
    "\n",
    "    openComment(browser)\n",
    "    getBack(browser)\n",
    "    \n",
    "    for r in range(3):\n",
    "        openReply(browser)\n",
    "        getBack(browser)\n",
    "    \n",
    "    for r in range(3):\n",
    "        openReply2(browser)\n",
    "        getBack(browser)\n",
    "    \n",
    "    openSeeMore(browser)\n",
    "    getBack(browser)\n",
    "    \n",
    "    # scroll to the bottom\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # process check\n",
    "    reviewList = browser.find_elements_by_xpath(\"//div[@class='du4w35lb k4urcfbm l9j0dhe7 sjgh65i0']\")\n",
    "    numReviews = len(reviewList)\n",
    "    if old_numReviews < numReviews:\n",
    "        print('Scroll Count:', count, '  numReviews:', numReviews)\n",
    "    old_numReviews = numReviews\n",
    "    \n",
    "    # termination condition\n",
    "    if numReviews >= specifiedNumber:\n",
    "        archiveAtEnd(browser, reviewList)\n",
    "        switch = False\n",
    "\n",
    "\n",
    "with open(f'test.html',\"r\", encoding=\"utf-8\") as file:\n",
    "    f = file.read()\n",
    "\n",
    "page = bs(f, 'lxml')\n",
    "reviews = page.find_all('div', {\n",
    "    'class':'du4w35lb k4urcfbm l9j0dhe7 sjgh65i0'\n",
    "                               })\n",
    "\n",
    "ratings = []\n",
    "users = []\n",
    "usernames = []\n",
    "texts = []\n",
    "dates = []\n",
    "for idx,r in enumerate(reviews):\n",
    "    rating = r.find('h2',{\"class\":\"gmql0nx0 l94mrbxd p1ri9a11 lzcic4wl aahdfvyu hzawbc8m\"}).get_text()\n",
    "    try:\n",
    "        ratings.append([i.strip() for i in rating.split('\\n') if 'recommend' in i][0])\n",
    "    except:\n",
    "        ratings.append('no rating')\n",
    "\n",
    "    users.append(r.find('a',{'class':'oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl oo9gr5id gpro0wi8 lrazzd5p'}).get_text().strip())\n",
    "    \n",
    "    username = r.find('a',{'class':'oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl oo9gr5id gpro0wi8 lrazzd5p'})\\\n",
    "                 .get('href')[25:].split('?')[0]\n",
    "    if username == 'profile.php':\n",
    "        username = r.find('a',{'class':'oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl oo9gr5id gpro0wi8 lrazzd5p'})\\\n",
    "                 .get('href').split('?id=')[1].split('&')[0]\n",
    "    usernames.append(username)\n",
    "    \n",
    "    text = r.find('span',{'class':'d2edcug0 hpfvmrgz qv66sw1b c1et5uql oi732d6d ik7dh3pa ht8s03o8 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d9wwppkn fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v b1v8xokw oo9gr5id hzawbc8m'})\n",
    "    if text is not None:\n",
    "        texts.append(' '.join([i.strip() for i in text.get_text().split()]))\n",
    "    else:\n",
    "        text = r.find('div',{'class':'kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql'})\n",
    "        if text is not None:\n",
    "            texts.append(text.get_text().strip())\n",
    "        else:\n",
    "            texts.append('no text')\n",
    "            \n",
    "    date = r.find('a',{'class':'oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl gmql0nx0 gpro0wi8 b1v8xokw'}).get('aria-label')\n",
    "    if (' ' not in date) and ('h' in date):\n",
    "        dates.append('October 1, 2021') #### REMEMBER TO CHANGE\n",
    "    elif (' ' not in date) and ('d' in date):\n",
    "        date0 = datetime.datetime.strptime('10/01/21', \"%m/%d/%y\")\n",
    "        date1 = date0 - datetime.timedelta(days=int(date[0]))\n",
    "        dates.append(date1)\n",
    "    elif ('2020' in date) or ('2019' in date) or ('2018' in date):\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        dates.append(' '.join(date.split()[:2])+', 2021')\n",
    "\n",
    "master = pd.DataFrame({\n",
    "    'ratings':ratings, \n",
    "    'users':users,\n",
    "    'usernames':usernames,\n",
    "    'texts':texts,\n",
    "    'dates':dates\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c73761719d425b2cb215bb8693e11bb404656c658de8c82d215c47a7c086c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
